# -*- coding: utf-8 -*-
"""Task: Build the Full RAG UI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MznJhoyVzOPxDELHUhtRbm99t-wxSH3i

# Full RAG UI

## Packages
"""

# Install required packages
!pip install -q gradio
!pip install -q gradio_pdf
!pip install -q pypdf PyPDF2 pymupdf
!pip install -q sentence-transformers transformers
!pip install -q faiss-cpu
!pip install -q google-generativeai
!pip install -q numpy pandas

# Install LlamaIndex packages for enhanced document processing
!pip install -q llama-index
!pip install -q llama-index-readers-file
!pip install -q llama-index-embeddings-huggingface
!pip install -q llama-index-vector-stores-faiss
!pip install -q llama-index-llms-llama-cpp
!pip install -q llama-cpp-python
!pip install -q sentencepiece

"""## üîß Core Imports and Configuration"""

import gradio as gr
from gradio_pdf import PDF
import fitz  # PyMuPDF
import numpy as np
from sentence_transformers import SentenceTransformer
import faiss
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json
from datetime import datetime
import hashlib

from llama_index.core import SimpleDirectoryReader
from llama_index.core import VectorStoreIndex, Document
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.settings import Settings
from llama_index.llms.llama_cpp import LlamaCPP
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.readers.file import PDFReader
from llama_cpp import Llama

from llama_cpp import Llama

llm = Llama.from_pretrained(
    repo_id="Qwen/Qwen3-4B-GGUF",
    filename="Qwen3-4B-Q4_K_M.gguf",
    n_ctx=2048,
    n_threads=8,
    n_gpu_layers=-1
)

import re

def clean_response(response):
    """
    Accept either:
      - dict with ['choices'][0]['message']['content']  (GenAI style)
      - dict with ['choices'][0]['text'] (llama-cpp-python older)
      - an object with .text or .content fields
      - plain string
    Always return a plain string with hidden reasoning removed.
    """
    text = ""
    try:
        if isinstance(response, dict):
            # google/other style
            if "choices" in response and response["choices"]:
                choice = response["choices"][0]
                if isinstance(choice.get("message"), dict):
                    text = choice["message"].get("content", "")
                else:
                    text = choice.get("text", "")
            elif "text" in response:
                text = response["text"]
            else:
                # fallback
                text = str(response)
        else:
            # object with .text or .content
            text = getattr(response, "text", None) or getattr(response, "content", None) or str(response)
    except Exception:
        text = str(response)

    # remove hidden reasoning tags like <think>...</think>
    cleaned_output = re.sub(r"<think>.*?</think>", "", text, flags=re.DOTALL).strip()
    return cleaned_output

"""## üìÑ Data Structures for Enhanced Document Management
Let's define our data structures to handle complex document metadata:
"""

@dataclass
class PageInfo:
    """Stores information about a single page"""
    page_num: int
    text: str
    doc_type: Optional[str] = None
    page_in_doc: int = 0

@dataclass
class LogicalDocument:
    """Represents a logical document within a PDF"""
    doc_id: str
    doc_type: str
    page_start: int
    page_end: int
    text: str
    chunks: List[Dict] = None

@dataclass
class ChunkMetadata:
    """Rich metadata for each chunk"""
    chunk_id: str
    doc_id: str
    doc_type: str
    chunk_index: int
    page_start: int
    page_end: int
    text: str
    embedding: Optional[np.ndarray] = None

from sentence_transformers import SentenceTransformer
embed_model = SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

"""## üß† Document Intelligence Functions
These functions handle document classification and boundary detection:
"""

def clean_text_for_llm(text: str) -> str:
    """Clean text for LLM classification: remove extra symbols and long whitespace."""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s,.@/-]', '', text)
    return text.strip()

def classify_document_type(text: str, max_length: int = 800) -> str:
    """
    Robust and deterministic document classifier with few-shot examples
    (especially for mortgage documents).
    """
    text_sample = text[:max_length]


    prompt = f"""
      You are a precise document classifier.
      Choose exactly one label from this list:

      Resume, Employment Contract, Contract, Mortgage Contract, Lender Fee Sheet,
      Invoice, Pay Slip, Land Deed, Bank Statement, Tax Document, Insurance,
      Report, Letter, Form, ID Document, Medical, Other.

      Here are some examples of how to classify:

      Example 1:
      "This document lists education, experience, and skills of a person applying for a job."
      ‚Üí Resume

      Example 2:
      "This agreement outlines the rights and obligations between two parties."
      ‚Üí Contract

      Example 3:
      "This document provides loan or mortgage fee details, rates, and payment summaries."
      ‚Üí Lender Fee Sheet

      Example 4:
      "This document describes the terms and details of a home loan or property mortgage."
      ‚Üí Mortgage Contract

      Example 5:
      "This file includes itemized charges or billing information for goods or services."
      ‚Üí Invoice

      Example 6:
      "If it‚Äôs a job offer, employment, or terms between employer and employee ‚Üí Employment Contract"

      Now classify the following text:

      {text_sample}

      Respond only with the label name.
    """

    try:
        response = llm.create_completion(prompt, temperature=0, top_k=2, top_p=1)
        text_out = clean_response(response).strip().split("\n")[0].strip().lower()

        valid = {
            "resume": "Resume",
            "employment contract": "Employment Contract",
            "invoice": "Invoice",
            "pay slip": "Pay Slip",
            "lender fee sheet": "Lender Fee Sheet",
            "mortgage contract": "Mortgage Contract",
            "land deed": "Land Deed",
            "bank statement": "Bank Statement",
            "tax document": "Tax Document",
            "insurance": "Insurance",
            "report": "Report",
            "letter": "Letter",
            "form": "Form",
            "id document": "ID Document",
            "medical": "Medical",
            "other": "Other"
        }

        for key, val in valid.items():
            if key in text_out:
                return val

        lower_text = text.lower()
        if any(word in lower_text for word in ["experience", "education", "skills", "intern", "bachelor", "university", "resume", "cv"]):
            return "Resume"
        if any(word in lower_text for word in ["loan", "mortgage", "closing disclosure", "fee worksheet", "annual percentage rate", "principal and interest"]):
            return "Lender Fee Sheet"
        if any(word in lower_text for word in ["agreement", "party", "contract", "terms and conditions", "employment"]):
            return "Employment Contract"

        return "Other"

    except Exception as e:
        print("Classification error:", e)
        return "Other"


def detect_document_boundary(prev_text: str, curr_text: str, current_doc_type: str = None) -> bool:
    """
    Detect if two consecutive pages belong to the same document.
    Returns True if they're from the same document.
    """
    if not prev_text or not curr_text:
        return False

    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text
    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text

    prompt = f"""
      You decide if Page_2 starts a NEW document, given Page_1 and its document type.Output exactly 'yes' or 'no'. No other text.

      Current document type: {current_doc_type or 'Unknown'}

      End of Previous Page:
      {prev_sample}

      Start of Current Page:
      {curr_sample}

      Answer with only one word: Yes or No.
    """

    try:
        response = llm.create_completion(prompt)
        output = clean_response(response).strip().lower()
        return output.startswith("yes")
    except Exception as e:
        print(f"Boundary detection error: {e}")
        return True  # assume same document on failure

"""## üìë Advanced PDF Processing Pipeline
Now let's build the enhanced PDF processing pipeline:
"""

!pip install pytesseract
!pip install pdfplumber

import fitz  # PyMuPDF
import pdfplumber
import pytesseract
from PIL import Image
import io

def extract_text_from_pdf_smart(pdf_file):
    """
    High-quality, fallback-based PDF text extractor.
    Combines PyMuPDF + pdfplumber + OCR if needed.
    """
    print("üìÑ Extracting text...")

    # Open file stream
    if hasattr(pdf_file, "read"):
        doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    else:
        doc = fitz.open(pdf_file)

    pages_text = []
    for i, page in enumerate(doc):
        text = page.get_text("text").strip()
        if len(text) < 50:  # suspiciously empty
            print(f"‚ö†Ô∏è Page {i+1}: No text detected, trying pdfplumber...")
            try:
                with pdfplumber.open(pdf_file) as alt_doc:
                    alt_page = alt_doc.pages[i]
                    text = alt_page.extract_text() or ""
            except Exception as e:
                print(f"pdfplumber failed: {e}")

        if len(text) < 50:  # still empty ‚Üí OCR
            print(f"üß† Page {i+1}: Using OCR...")
            try:
                pix = page.get_pixmap(dpi=200)
                img = Image.open(io.BytesIO(pix.tobytes("png")))
                text = pytesseract.image_to_string(img)
            except Exception as e:
                print(f"OCR failed on page {i+1}: {e}")
                text = ""

        text = text.strip().replace("\n", " ").replace("  ", " ")
        pages_text.append(text)

    doc.close()

    print(f"‚úÖ Extracted text from {len(pages_text)} pages.")
    return pages_text

def extract_and_analyze_pdf(pdf_file):
    """
    Improved multi-document detection for mixed PDF blobs.
    Splits whenever the document type changes.
    """
    pages_info = extract_text_from_pdf_smart(pdf_file)  # your existing OCR/text extraction function
    logical_docs = []

    prev_doc_type = None
    current_doc_pages = []
    current_text = ""

    for i, page_text in enumerate(pages_info):
        clean_text = page_text.strip().replace("\n", " ")
        doc_type = classify_document_type(clean_text)

        print(f"üìÑ Page {i}: Classified as {doc_type}")

        if doc_type != prev_doc_type and prev_doc_type is not None:
            # save the previous document
            logical_docs.append(LogicalDocument(
                doc_id=f"doc_{len(logical_docs)+1}",
                doc_type=prev_doc_type,
                text=current_text.strip(),
                page_start=i - len(current_doc_pages),
                page_end=i - 1
            ))
            # reset accumulators
            current_doc_pages = []
            current_text = ""

        # accumulate page text
        current_doc_pages.append(i)
        current_text += clean_text + "\n"
        prev_doc_type = doc_type

    # save the last document
    if current_text.strip():
        logical_docs.append(LogicalDocument(
            doc_id=f"doc_{len(logical_docs)+1}",
            doc_type=prev_doc_type or "Unknown",
            text=current_text.strip(),
            page_start=len(pages_info) - len(current_doc_pages),
            page_end=len(pages_info) - 1
        ))

    print(f"‚úÖ Identified {len(logical_docs)} logical documents:")
    for doc in logical_docs:
        print(f"   - {doc.doc_type}: Pages {doc.page_start}-{doc.page_end}")

    return pages_info, logical_docs

"""## ‚úÇÔ∏è Intelligent Chunking with Metadata Preservation
We'll provide two chunking approaches - our custom implementation and LlamaIndex's built-in capabilities:
"""

from llama_index.core.node_parser import SentenceSplitter

def chunk_document_with_metadata(logical_doc: LogicalDocument,
                                chunk_size: int = 300,
                                overlap: int = 80) -> List[ChunkMetadata]:
    """
    Chunk a logical document while preserving rich metadata.
    Uses sliding window with overlap for better context.
    """
    chunks_metadata = []
    words = logical_doc.text.split()

    if len(words) <= chunk_size:
        # Document is small enough to be a single chunk
        chunk_meta = ChunkMetadata(
            chunk_id=f"{logical_doc.doc_id}_chunk_0",
            doc_id=logical_doc.doc_id,
            doc_type=logical_doc.doc_type,
            chunk_index=0,
            page_start=logical_doc.page_start,
            page_end=logical_doc.page_end,
            text=logical_doc.text
        )
        chunks_metadata.append(chunk_meta)
    else:
        # Create overlapping chunks
        stride = chunk_size - overlap
        for i, start_idx in enumerate(range(0, len(words), stride)):
            end_idx = min(start_idx + chunk_size, len(words))
            chunk_text = ' '.join(words[start_idx:end_idx])

            # Calculate which pages this chunk spans
            # (simplified - in production, track more precisely)
            chunk_position = start_idx / len(words)
            page_range = logical_doc.page_end - logical_doc.page_start
            relative_page = int(chunk_position * page_range)
            chunk_page_start = logical_doc.page_start + relative_page
            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)

            chunk_meta = ChunkMetadata(
                chunk_id=f"{logical_doc.doc_id}_chunk_{i}",
                doc_id=logical_doc.doc_id,
                doc_type=logical_doc.doc_type,
                chunk_index=i,
                page_start=chunk_page_start,
                page_end=chunk_page_end,
                text=chunk_text
            )
            chunks_metadata.append(chunk_meta)

            if end_idx >= len(words):
                break

    return chunks_metadata

def chunk_with_llama_index(logical_doc: LogicalDocument,
                           chunk_size: int = 300,
                           chunk_overlap: int = 80) -> List[Document]:
    """
    Alternative: Use LlamaIndex's advanced chunking with metadata.
    """
    # Create LlamaIndex document with metadata
    doc = Document(
        text=logical_doc.text,
        metadata={
            "doc_id": logical_doc.doc_id,
            "doc_type": logical_doc.doc_type,
            "page_start": logical_doc.page_start,
            "page_end": logical_doc.page_end,
            "source": f"{logical_doc.doc_type}_document"
        }
    )

    # Use LlamaIndex's sentence splitter for better chunking
    splitter = SentenceSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        paragraph_separator="\n\n",
        separator=" ",
    )

    # Create nodes (chunks) from document
    nodes = splitter.get_nodes_from_documents([doc])

    # Convert to our ChunkMetadata format for consistency
    chunks_metadata = []
    for i, node in enumerate(nodes):
        chunk_meta = ChunkMetadata(
            chunk_id=f"{logical_doc.doc_id}_chunk_{i}",
            doc_id=logical_doc.doc_id,
            doc_type=logical_doc.doc_type,
            chunk_index=i,
            page_start=node.metadata.get("page_start", logical_doc.page_start),
            page_end=node.metadata.get("page_end", logical_doc.page_end),
            text=node.text
        )
        chunks_metadata.append(chunk_meta)

    return chunks_metadata

def process_all_documents(logical_docs: List[LogicalDocument]):

    """
    Process all logical documents into structured chunks with metadata.
    """
    all_chunks = []

    for logical_doc in logical_docs:
        if not logical_doc.text.strip():
            continue

        chunks = chunk_with_llama_index(logical_doc)

        for chunk in chunks:
            if isinstance(chunk, str):
                chunk = ChunkMetadata(
                    chunk_id=f"{logical_doc.doc_id}_chunk_fallback",
                    doc_id=logical_doc.doc_id,
                    doc_type=logical_doc.doc_type,
                    chunk_index=0,
                    page_start=logical_doc.page_start,
                    page_end=logical_doc.page_end,
                    text=chunk
                )
            all_chunks.append(chunk)

        logical_doc.chunks = chunks  # attach for UI display
        print(f"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks")

    return all_chunks

"""## üéØ Query Routing and Intelligent Retrieval"""

def predict_query_document_type(query: str) -> Tuple[str, float]:
    """
    Predict which document type is most likely to contain the answer.
    Returns predicted type and confidence score.
    """
    prompt = f"""
    Analyze this query and predict which document type would most likely contain the answer.

    Query: "{query}"

    Choose the MOST LIKELY type from:
    - Resume: Career, experience, education, skills, employment history
    - Contract: Terms, agreements, obligations, parties, legal terms
    - Mortgage Contract: Loan, Home loan, property financing, mortgage terms, interest rates
    - Invoice: Payments, amounts due, billing, charges, invoiced items
    - Pay Slip: Salary, wages, deductions, earnings, pay period
    - Lender Fee Sheet: Loan fees, closing costs, origination fees, lender charges
    - Land Deed: Property ownership, deed information, property description, title
    - Bank Statement: Account balance, transactions, deposits, withdrawals
    - Tax Document: Tax information, W2, 1099, tax returns, tax amounts
    - Insurance: Coverage, policy details, premiums, claims
    - Report: Analysis, findings, conclusions, research data
    - Letter: Communications, requests, notifications, correspondence
    - Form: Applications, submitted data, form fields
    - ID Document: Personal identification, ID numbers, identity verification
    - Medical: Health information, medical conditions, prescriptions
    - Other: General or unclear

    Respond in JSON format:
    {{"type": "DocumentType", "confidence": 0.85}}

    Confidence should be between 0.0 and 1.0 /no_think
    """

    try:
        response = llm.create_chat_completion(prompt)
        output = clean_response(response)
        result = json.loads(output.text.strip())
        return result.get("type", "Other"), result.get("confidence", 0.5)
    except Exception as e:
        print(f"Query routing error: {e}")
        return "Other", 0.0

class IntelligentRetriever:
    """
    Advanced retrieval system with metadata filtering and query routing.
    """

    def __init__(self):
        self.index = None
        self.chunks_metadata = []
        self.doc_type_indices = {}  # Separate indices per doc type

    def build_indices(self, chunks_metadata: List[ChunkMetadata]):
      """
      Build FAISS indices and store references to chunk objects.
      """
      print("üî® Building vector indices...")
      self.chunks_metadata = chunks_metadata
      # Create embeddings
      texts = [chunk.text for chunk in chunks_metadata]
      embeddings = embed_model.encode(texts, show_progress_bar=True)

      for i, chunk in enumerate(chunks_metadata):
          chunk.embedding = embeddings[i]

      dim = embeddings.shape[1]
      self.index = faiss.IndexFlatL2(dim)
      self.index.add(embeddings)

      # Build per-type sub-indices
      self.doc_type_indices = {}
      for doc_type in set(c.doc_type for c in chunks_metadata):
          type_indices = [i for i, c in enumerate(chunks_metadata) if c.doc_type == doc_type]
          if not type_indices:
              continue

          sub_emb = embeddings[type_indices]
          sub_index = faiss.IndexFlatL2(dim)
          sub_index.add(sub_emb)

          self.doc_type_indices[doc_type] = {
              "index": sub_index,
              "mapping": type_indices
          }

      print(f"‚úÖ Indexed {len(chunks_metadata)} chunks across {len(self.doc_type_indices)} document types")

    def retrieve(self, query: str, k: int = 4,
                filter_doc_type: Optional[str] = None,
                auto_route: bool = True) -> List[Tuple[ChunkMetadata, float]]:
        """
        Retrieve relevant chunks with optional filtering and routing.
        Returns chunks with relevance scores.
        """
        query_embedding = embed_model.encode([query])

        # Determine which index to search
        if filter_doc_type and filter_doc_type in self.doc_type_indices:
            # Use filtered index
            type_data = self.doc_type_indices[filter_doc_type]
            D, I = type_data['index'].search(query_embedding, k)
            # Map back to original chunks
            chunk_indices = [type_data['mapping'][i] for i in I[0]]
            distances = D[0]
        elif auto_route:
            # Predict best document type
            predicted_type, confidence = predict_query_document_type(query)
            print(f"üéØ Query routed to: {predicted_type} (confidence: {confidence:.2f})")

            if confidence > 0.7 and predicted_type in self.doc_type_indices:
                # High confidence - use specific index
                type_data = self.doc_type_indices[predicted_type]
                D, I = type_data['index'].search(query_embedding, k)
                chunk_indices = [type_data['mapping'][i] for i in I[0]]
                distances = D[0]
            else:
                # Low confidence - search all
                D, I = self.index.search(query_embedding, k)
                chunk_indices = I[0]
                distances = D[0]
        else:
            # Search all chunks
            D, I = self.index.search(query_embedding, k)
            chunk_indices = I[0]
            distances = D[0]

        # Convert distances to similarity scores (inverse)
        max_dist = max(distances) if len(distances) > 0 else 1.0
        scores = [(max_dist - d) / max_dist for d in distances]

        results = [(self.chunks_metadata[i], scores[idx])
                  for idx, i in enumerate(chunk_indices)]

        return results

"""## üí¨ Enhanced Answer Generation with Source Attribution"""

import tiktoken

def truncate_to_token_limit(text, limit=1600):
    """Truncate text safely to stay within LLM context window."""
    try:
        enc = tiktoken.get_encoding("cl100k_base")
        tokens = enc.encode(text)
        return enc.decode(tokens[:limit])
    except Exception:
        # fallback: simple char-based truncation
        return text[:limit * 4]  # rough fallback (~4 chars/token)

def generate_answer_with_sources(query: str, retrieved_chunks: List[Tuple[ChunkMetadata, float]]) -> Dict:
    """
    Generate answer with detailed source attribution + debug-friendly prints for OCR, retrieval, and answer.
    """

    if not retrieved_chunks:
        print("‚ö†Ô∏è No chunks retrieved.")
        return {
            'answer': "I couldn't find relevant information to answer your question.",
            'sources': [],
            'confidence': 0.0
        }

    # ‚úÖ Combine top chunks safely (limit to top 4)
    context_parts = []
    sources = []
    print("\n" + "=" * 80)
    print(f"üß† Query: {query}")
    print("=" * 80)
    print("\nüìö Top Retrieved Chunks:")

    for i, (chunk_meta, score) in enumerate(retrieved_chunks[:4]):
        chunk_text = getattr(chunk_meta, "text", str(chunk_meta))
        context_parts.append(chunk_text)

        print(f"\n--- Chunk {i+1} ---")
        print(f"üìÑ Doc Type: {chunk_meta.doc_type}")
        print(f"üìë Pages: {chunk_meta.page_start}-{chunk_meta.page_end}")
        print(f"üéØ Similarity: {score:.2%}")
        print(f"üìù Text Preview: {chunk_text[:300].strip()}...")

        sources.append({
            'doc_type': chunk_meta.doc_type,
            'pages': f"{chunk_meta.page_start}-{chunk_meta.page_end}",
            'relevance': f"{score:.2%}",
            'preview': chunk_text[:100] + "..."
        })

    # ‚úÖ Build combined context (limit tokens to prevent overflow)
    context = "\n\n".join(context_parts)
    context = truncate_to_token_limit(context, limit=1600)

    print("\n" + "=" * 80)
    print(f"üìñ Context Length: {len(context)} chars (truncated to fit context window)")
    print("=" * 80)

    prompt = f"""
You are a helpful AI assistant. Use the provided context to answer the question clearly and concisely.

Context:
{context}

Question: {query}

Answer:
"""

    try:
        if hasattr(llm, "create_completion"):
            response = llm.create_completion(prompt)
        else:
            response = llm.create_chat_completion(prompt)

        answer = clean_response(response)

        print("\n" + "=" * 80)
        print("ü§ñ Final LLM Answer:")
        print("=" * 80)
        print(answer)
        print("=" * 80)

        avg_score = sum(s for _, s in retrieved_chunks) / len(retrieved_chunks)
        return {
            'answer': answer,
            'sources': sources,
            'confidence': avg_score,
            'chunks_used': len(retrieved_chunks)
        }

    except Exception as e:
        print("‚ùå Answer generation error:", e)
        return {
            'answer': f"Error generating answer: {str(e)}",
            'sources': sources,
            'confidence': 0.0
        }

"""## üèóÔ∏è Enhanced Document Store"""

class EnhancedDocumentStore:
    """
    Manages the complete document processing and retrieval pipeline.
    """

    def __init__(self):
        self.pages_info = []
        self.logical_docs = []
        self.chunks_metadata = []
        self.retriever = IntelligentRetriever()
        self.is_ready = False
        self.processing_stats = {}
        self.filename = None

    def process_pdf(self, pdf_file, filename: str = "document.pdf"):
        """
        Complete PDF processing pipeline.
        """
        self.filename = filename
        self.is_ready = False
        start_time = datetime.now()

        try:
            # Extract and analyze PDF
            self.pages_info, self.logical_docs = extract_and_analyze_pdf(pdf_file)

            # --- Show sample OCR output for reporting ---
            if self.pages_info and 'text' in self.pages_info[0]:
                print("\nüßæ OCR Output (first 400 characters):")
                print(self.pages_info[0]['text'][:400])
                print("... [OCR output truncated for readability]\n")

            # Chunk documents with metadata
            self.chunks_metadata = process_all_documents(self.logical_docs)

            # Build retrieval indices
            self.retriever.build_indices(self.chunks_metadata)

            # Calculate processing statistics
            process_time = (datetime.now() - start_time).total_seconds()
            self.processing_stats = {
                'filename': filename,
                'total_pages': len(self.pages_info),
                'documents_found': len(self.logical_docs),
                'total_chunks': len(self.chunks_metadata),
                'document_types': list(set(doc.doc_type for doc in self.logical_docs)),
                'processing_time': f"{process_time:.1f}s"
            }

            self.is_ready = True
            return True, self.processing_stats

        except Exception as e:
            return False, {'error': str(e)}

    def query(self, question: str, filter_type: Optional[str] = None,
             auto_route: bool = True, k: int = 4) -> Dict:
        """
        Query the document store.
        """
        if not self.is_ready:
            return {
                'answer': "Please upload and process a PDF first.",
                'sources': [],
                'confidence': 0.0
            }

        # Retrieve relevant chunks
        retrieved = self.retriever.retrieve(
            question, k=k,
            filter_doc_type=filter_type,
            auto_route=auto_route
        )

        # --- Show retrieved chunks for reporting ---
        print("\nüéØ Retrieved Chunks (Top Results):")
        for i, (chunk, score) in enumerate(retrieved[:min(3, len(retrieved))]):
            print(f"{i+1}. {chunk.doc_type} (Pages {chunk.page_start}-{chunk.page_end}) | Relevance: {score:.2%}")
            snippet = getattr(chunk, 'text', str(chunk))[:300].replace('\n', ' ')
            print(f"   Preview: {snippet}...\n")

        # Generate answer with sources
        result = generate_answer_with_sources(question, retrieved)
        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')

        return result

    def get_document_structure(self) -> List[Dict]:
        """
        Get the document structure for UI display.
        """
        if not self.logical_docs:
            return []

        structure = []
        for doc in self.logical_docs:
            structure.append({
                'id': doc.doc_id,
                'type': doc.doc_type,
                'pages': f"{doc.page_start + 1}-{doc.page_end + 1}",  # 1-indexed for UI
                'chunks': len(doc.chunks) if doc.chunks else 0,
                'preview': doc.text[:200] + "..." if len(doc.text) > 200 else doc.text
            })

        return structure

pages = extract_text_from_pdf_smart("Anjum_resume.pdf")
for i, t in enumerate(pages[:3]):
    print(f"\n--- PAGE {i+1} ---\n", t[:500])

text="our actual rate, payment, and cost could be higher. Get an official Loan Estimate before choosing a loan. Fee Details and Summary Applicants: Application No: Date Prepared: Loan Program: Prepared By: THIS IS NOT A GOOD FAITH ESTIMATE (GFE). This Fees Worksheet is provided for informational purposes ONLY, to assist you in determining an estimate of cash that may be required to close and an estimate of your proposed monthly mortgage payment. Actual charges may be more or less, and your transact"
classify_document_type(text)

text = "Personal Summary I started my coding journey at 18 and have since expanded my skills in areas like Data Analysis, Artificial Intelligence, and software development. Along the way, I‚Äôve had the opportunity to teach Python, participate in data-driven projects, and engage in programs that combine both technical and practical learning. I‚Äôm passionate about using technology to solve real-world problems and always eager to learn and grow in the fast-evolving tech Universiti Teknologi Malaysia ‚Ä¢ Bachelor Of Computer Science (Software Engineering) With Honours Experience AI/ML Engineering Intern "
classify_document_type(text)

"""## üé® Gradio Interface with Enhanced Features
Now let's create the sophisticated Gradio interface:
"""

# Global store instance
doc_store = EnhancedDocumentStore()

def process_pdf_handler(pdf_file):
    """Handle PDF upload and processing."""
    if pdf_file is None:
        return "‚ö†Ô∏è Please upload a PDF file", None, gr.update(choices=["All"])

    # Process the PDF

    success, stats = doc_store.process_pdf(pdf_file,filename=getattr(pdf_file, 'name', 'document.pdf'))

    if success:
        # Prepare status message
        status_msg = f"""
        ‚úÖ **Successfully Processed:**
        - üìÑ File: {stats['filename']}
        - üìë Pages: {stats['total_pages']}
        - üìö Documents Found: {stats['documents_found']}
        - üß© Chunks Created: {stats['total_chunks']}
        - üè∑Ô∏è Types: {', '.join(stats['document_types'])}
        - ‚è±Ô∏è Time: {stats['processing_time']}
        """

        # Get document structure for display
        structure = doc_store.get_document_structure()
        structure_display = "\n".join([
            f"‚Ä¢ **{doc['type']}** (Pages {doc['pages']}): {doc['chunks']} chunks"
            for doc in structure
        ])

        # Update filter choices
        doc_types = ["All"] + stats['document_types']

        return status_msg, structure_display, gr.update(choices=doc_types, value="All")
    else:
        return f"‚ùå Error: {stats.get('error', 'Unknown error')}", None, gr.update(choices=["All"])

import time

# Create a simple log to store timing info
performance_log = {
    "retrieval_times": [],
    "llm_times": [],
    "confidence_scores": []
}

def chat_handler(message, history, doc_filter, auto_route, num_chunks):
    """Handle chat interactions with performance tracking (safe version)."""
    if not doc_store.is_ready:
        response = "üìö Please upload and process a PDF document first."
        return history + [[message, response]]

    filter_type = None if doc_filter == "All" else doc_filter

    # Measure retrieval + generation time together
    start_time = time.time()
    result = doc_store.query(
        message,
        filter_type=filter_type,
        auto_route=auto_route and filter_type is None,
        k=num_chunks
    )
    end_time = time.time()

    # Log metrics (non-intrusive)
    total_time = end_time - start_time
    performance_log["retrieval_times"].append(total_time)
    performance_log["confidence_scores"].append(result.get("confidence", 0))

    # Build response
    response = f"{result['answer']}\n\n"

    if result['sources']:
        response += "üìç **Sources:**\n"
        for src in result['sources']:
            response += f"‚Ä¢ {src['doc_type']} (Pages {src['pages']}) - Relevance: {src['relevance']}\n"

    response += f"\n*Confidence: {result['confidence']:.1%} | Response Time: {total_time:.2f}s | Filter: {result['filter_used']}*"

    return history + [[message, response]]

def create_interface():
    """Create the enhanced Gradio interface with cyan-themed styling (no breaking changes)."""

    custom_css = """
    #chatbot {
        border: 2px solid #00bcd4;
        border-radius: 10px;
    }
    #status_bar {
        background-color: #e0f7fa;
        color: #004d40;
        border-radius: 8px;
        padding: 8px;
        font-weight: 600;
    }
    .gr-button-primary {
        background-color: #00acc1 !important;
        color: white !important;
        border-radius: 8px !important;
    }
    .gr-button-secondary {
        background-color: #b2ebf2 !important;
        color: #004d40 !important;
        border-radius: 8px !important;
    }
    """

    with gr.Blocks(
        title="Enhanced Document Q&A",
        theme=gr.themes.Soft(primary_hue="cyan", secondary_hue="gray", neutral_hue="slate"),
        css=custom_css
    ) as demo:
        gr.Markdown(
            """
            <div style='text-align:center; padding:15px 0'>
                <h1 style='color:#0097a7;'>üöÄ Enhanced Document Q&A System</h1>
                <h3 style='color:#00796b;'>Intelligent Multi-Document Analysis with Advanced RAG Pipeline</h3>
            </div>
            """
        )

        with gr.Row():
            # Left side - PDF preview and upload
            with gr.Column(scale=2):
                pdf_input = PDF(
                    label="üìÑ PDF Document Viewer",
                    interactive=True,
                    height=600
                )

                with gr.Row():
                    process_btn = gr.Button(
                        "üîÑ Process Document",
                        variant="primary",
                        size="lg",
                        scale=2
                    )
                    clear_all_btn = gr.Button(
                        "üóëÔ∏è Clear All",
                        variant="secondary",
                        size="lg",
                        scale=1
                    )

            # Middle - Document info and settings
            with gr.Column(scale=1):
                gr.Markdown("<h3 style='color:#006064;'>üìä Document Info</h3>")
                status_output = gr.Markdown(value="‚è≥ Waiting for PDF upload...")

                structure_output = gr.Markdown(value="", label="Document Structure")

                gr.Markdown("<h3 style='color:#006064;'>‚öôÔ∏è Settings</h3>")

                doc_filter = gr.Dropdown(
                    choices=["All"],
                    value="All",
                    label="üè∑Ô∏è Document Type Filter"
                )

                auto_route = gr.Checkbox(
                    value=True,
                    label="üéØ Auto-Route Queries"
                )

                num_chunks = gr.Slider(
                    minimum=1,
                    maximum=10,
                    value=4,
                    step=1,
                    label="üìä Chunks to Retrieve"
                )

            # Right side - Chat interface
            with gr.Column(scale=2):
                gr.Markdown("<h3 style='color:#006064;'>üí¨ Ask Questions</h3>")
                chatbot = gr.Chatbot(
                    label="Conversation",
                    height=500,
                    elem_id="chatbot",
                    show_label=False
                )

                with gr.Row():
                    msg_input = gr.Textbox(
                        label="Ask a question",
                        placeholder="e.g., What are the payment terms? What is the total amount?",
                        scale=4,
                        show_label=False
                    )
                    send_btn = gr.Button("üì§ Send", scale=1, variant="primary")

                with gr.Row():
                    clear_chat_btn = gr.Button("üóëÔ∏è Clear Chat", size="sm", scale=1)
                    example_btn1 = gr.Button("üìù What's the summary?", size="sm", scale=1)
                    example_btn2 = gr.Button("üí∞ Find amounts", size="sm", scale=1)

        # Status bar at the bottom
        with gr.Row():
            status_bar = gr.Markdown(
                value="**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0",
                elem_id="status_bar"
            )

        # Event handlers (identical to before)
        def update_status_bar():
            if doc_store.is_ready:
                stats = doc_store.processing_stats
                cache_rate = 0
                if hasattr(doc_store.retriever, 'total_queries') and doc_store.retriever.total_queries > 0:
                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100
                return f"**Status:** ‚úÖ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%"
            return "**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0"

        def clear_all():
            global doc_store
            doc_store = EnhancedDocumentStore()
            return (
                None,
                "‚è≥ Waiting for PDF upload...",
                "",
                gr.update(choices=["All"], value="All"),
                [],
                "",
                update_status_bar()
            )

        def process_pdf_with_status(pdf_file):
            status, structure, filter_update = process_pdf_handler(pdf_file)
            status_bar_text = update_status_bar()
            return status, structure, filter_update, status_bar_text

        def chat_with_status(message, history, doc_filter, auto_route, num_chunks):
            new_history = chat_handler(message, history, doc_filter, auto_route, num_chunks)
            status_bar_text = update_status_bar()
            return new_history, status_bar_text

        def ask_summary(history):
            return chat_handler("Can you provide a summary of the main points in this document?",
                                history, doc_filter.value, auto_route.value, num_chunks.value)

        def ask_amounts(history):
            return chat_handler("What are all the monetary amounts or financial figures mentioned?",
                                history, doc_filter.value, auto_route.value, num_chunks.value)

        # Wire up events (unchanged)
        process_btn.click(fn=process_pdf_with_status, inputs=[pdf_input],
                          outputs=[status_output, structure_output, doc_filter, status_bar])
        clear_all_btn.click(fn=clear_all,
                            outputs=[pdf_input, status_output, structure_output, doc_filter, chatbot, msg_input, status_bar])
        msg_input.submit(fn=chat_with_status,
                         inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],
                         outputs=[chatbot, status_bar]).then(lambda: "", outputs=[msg_input])
        send_btn.click(fn=chat_with_status,
                       inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],
                       outputs=[chatbot, status_bar]).then(lambda: "", outputs=[msg_input])
        clear_chat_btn.click(lambda: [], outputs=[chatbot])
        example_btn1.click(fn=ask_summary, inputs=[chatbot], outputs=[chatbot]).then(fn=update_status_bar, outputs=[status_bar])
        example_btn2.click(fn=ask_amounts, inputs=[chatbot], outputs=[chatbot]).then(fn=update_status_bar, outputs=[status_bar])
        pdf_input.change(fn=process_pdf_with_status, inputs=[pdf_input],
                         outputs=[status_output, structure_output, doc_filter, status_bar])

    return demo

demo = create_interface()
demo.launch(share=True, debug=True)

def show_performance_summary():
    """Show average response time and confidence."""
    if not performance_log["retrieval_times"]:
        print("No performance data yet ‚Äî run a few queries first.")
        return

    avg_time = sum(performance_log["retrieval_times"]) / len(performance_log["retrieval_times"])
    avg_conf = sum(performance_log["confidence_scores"]) / len(performance_log["confidence_scores"])

    print("üìä Pipeline Performance Summary")
    print("===================================")
    print(f"‚ö° Avg Response Time: {avg_time:.2f}s")
    print(f"üí¨ Avg Confidence: {avg_conf:.1%}")
    print(f"üß† Total Queries Tested: {len(performance_log['retrieval_times'])}")
    print("===================================")

show_performance_summary()